{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjS_p66DM6_O"
      },
      "source": [
        "### Group Members:\n",
        "\n",
        "- Hangchen Xie, 22-736-789\n",
        "- Yanyang Gong, 23-744-063\n",
        "- Yaojie Wang, 23-741-283\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsSWtTLDm0Lx"
      },
      "source": [
        "# Assignment 8: Open-Set Classification\n",
        "\n",
        "In this assignment, we develop a network that is capable of correctly classifying known classes while at the same time rejecting unknown samples that occur during inference time.\n",
        "To showcase the capability, we make use of the MNIST dataset that we artificially split into known and unknown classes; this allows us to train a network on the data without requiring too expensive hardware.\n",
        "\n",
        "\n",
        "We select the MNIST dataset and define several classes to be known, negative class during training, and unknown (not used for training at all).\n",
        "\n",
        "## Dataset\n",
        "We split the MNIST dataset into 4 known classes, 4 negative classes (used for training) and 2 unknown classes (used only for testing).\n",
        "While several splits might be possible, here we restrict to the following:\n",
        "* Known class indexes: (1, 4, 5, 8)\n",
        "* negative class indexes: (0, 2, 3, 7)\n",
        "* Unknown class indexes: (6,9)\n",
        "\n",
        "Please note that, in PyTorch, class indexing starts at 0 (other than in the lecture where class indexing starts at 1).\n",
        "\n",
        "We rely on the `torchvision.datasets.MNIST` implementation of the `MNIST` dataset, which we adapt to our needs.\n",
        "The constructor of our Dataset class takes one parameter that defines the purpose of this dataset (`\"train\", \"validation\", \"test\"`).\n",
        "The `\"train\"` partition uses the training samples of the *known* and the *negative* classes.\n",
        "The `\"validation\"` partition uses the test samples of the *known* and the *negative* classes.\n",
        "Finally, the `\"test\"` partition uses the test samples of the *known* and the *unknown* classes.\n",
        "\n",
        "In our implementation of the Dataset class, we need to implement two functions.\n",
        "* First, the constructor `__init__(self, purpose)` selects the data based on our purpose.\n",
        "* Second, the index function `__getitem__(self, n)` returns a pair $(X^n, \\vec t^n)$ for the sample with the index $n$, where $X \\in \\mathbb R^{1\\times28\\times28}$ with values in range $[0,1]$ and $\\vec t \\in \\mathbb R^{O}$, see below.\n",
        "\n",
        "Since our loss function (cf. Task 5) requires our target vectors to be in vector format, we need to convert the target index $t^n$ into its vector representation $\\vec t^n$.\n",
        "Particularly, we need to provide the following target vectors:\n",
        "\n",
        "<center>\n",
        "\n",
        " $\\vec t^n = 1 : \\vec t^n = (1,0,0,0)$\n",
        "\n",
        " $\\vec t^n = 4 : \\vec t^n = (0,1,0,0)$\n",
        "\n",
        " $\\vec t^n = 5 : \\vec t^n = (0,0,1,0)$\n",
        "\n",
        " $\\vec t^n = 8 : \\vec t^n = (0,0,0,1)$\n",
        "\n",
        " else: $\\vec t^n = (\\frac14,\\frac14,\\frac14,\\frac14)$\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "### Task 1: Target Vectors\n",
        "\n",
        "Implement a function that generates a target vector for any of the ten different classes according to above description. The return value should be a `torch.tensor` of type float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKlkqz_ym0L2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# define the three types of classes\n",
        "known_classes = (1,4,5,8)\n",
        "negative_classes = (0,2,3,7)\n",
        "unknown_classes = (6,9)\n",
        "O = len(known_classes)\n",
        "\n",
        "# define one-hot vectors\n",
        "labels_known = torch.eye(O) #2-D tensor with ones on the diagonal and zeros else where,n:rows\n",
        "label_unknown = torch.full((O,),fill_value=1/O) #tensor of size filled with fill_value\n",
        "\n",
        "def target_vector(index):\n",
        "  # select correct one-hot vector for known classes, and the 1/O-vectors for unknown classes\n",
        "  if index in known_classes:\n",
        "    return labels_known[known_classes.index(index)]\n",
        "  elif index in unknown_classes + negative_classes:\n",
        "    return label_unknown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFSme-RNm0L4"
      },
      "source": [
        "### Test 1: Check your Target Vectors\n",
        "\n",
        "Test that your target vectors are correct, for all tpyes of known and unknown samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn9oEs61m0L4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27daac3-c024-40da-8878-a7bc982b76a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 tensor([1., 0., 0., 0.])\n",
            "4 tensor([0., 1., 0., 0.])\n",
            "5 tensor([0., 0., 1., 0.])\n",
            "8 tensor([0., 0., 0., 1.])\n",
            "0 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "2 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "3 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "7 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "6 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "9 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
          ]
        }
      ],
      "source": [
        "# check that the target vectors for known classes are correct\n",
        "for index in known_classes:\n",
        "  t = target_vector(index)\n",
        "  print(index, t)\n",
        "  assert max(t) == 1\n",
        "  assert sum(t) == 1\n",
        "\n",
        "# check that the target vectors for unknown classes are correct\n",
        "for index in negative_classes + unknown_classes:\n",
        "  t = target_vector(index)\n",
        "  print(index, t)\n",
        "  assert max(t) == 0.25\n",
        "  assert sum(t) == 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPmC7414m0L5"
      },
      "source": [
        "### Tasks 2 and 3: Dataset Construction and Dataset Item Selection\n",
        "\n",
        "Write a dataset class that derives from `torchvision.datasets.MNIST` in `PyTorch` and adapts some parts of it.\n",
        "In the constructor, make sure that you let `PyTorch` load the dataset by calling the base class constructor `super` with the desired parameters. Afterward, the `self.data` and `self.targets` are populated with all samples and target indexes.\n",
        "From these, we need to sub-select the samples that fit our current `purpose` and store them back to `self.data` and `self.targets`.\n",
        "\n",
        "Second, we need to implement the index function of our dataset, where we need to return both the image and the target vector.\n",
        "The images in `self.data` were originally stored as `uint8` values in the dimension $\\mathbb N^{N\\times28\\times28}$ with values in $[0, 255]$.\n",
        "The targets in `self.targets` were originally stored as class indexes in the dimension $\\mathbb N^N$. Make sure that you return both in the desired format.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* Since Jupyter Notebook does not allow splitting classes over several code boxes, the two tasks are required to be solved in the same code box.\n",
        "* **The definition below is just one possibility.** There are many ways to implement this dataset interface.\n",
        "* With a clever implementation of the constructor, there is no need to overwrite the `__getitem__(self,index)` function.\n",
        "* Depending on your implementation, you might also need to overwrite the `__len__(self)` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLuxLw67l4Lu"
      },
      "outputs": [],
      "source": [
        "class DataSet(torchvision.datasets.MNIST):\n",
        "  def __init__(self, purpose=\"train\"):\n",
        "    # call base class constructor to handle the data loading\n",
        "    # make sure that you get the correct part of the data based on the purpose\n",
        "\n",
        "    super(DataSet, self).__init__(root='./data', train=(purpose != \"test\"), download=True)\n",
        "\n",
        "    # select the valid classes based on the current purpose\n",
        "    if purpose == \"train\" or purpose == \"validation\":\n",
        "      classes = (0, 1, 2, 3, 4, 5, 7, 8)  # Known and negative classes for training and validation\n",
        "    elif purpose == \"test\":\n",
        "      classes = (1, 4, 5, 8, 6, 9)  # Known and unknown classes for testing\n",
        "    else:\n",
        "      raise ValueError(\"Invalid purpose\")\n",
        "    # select the samples that belong to these classes\n",
        "    selected_indices = torch.tensor([i for i, target in enumerate(self.targets) if target in classes])\n",
        "\n",
        "    # sub-select the data of valid classes\n",
        "    self.data = self.data[selected_indices]\n",
        "    self.targets = self.targets[selected_indices]\n",
        "\n",
        "    self.data = self.data.float() / 255.0\n",
        "\n",
        "    # select the targets of valid classes, and already turn them into target vectors\n",
        "    self.target_vectors = torch.zeros((len(self.targets), O))\n",
        "    for i,target in enumerate(self.targets):\n",
        "      self.target_vectors[i] = target_vector(target)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # perform appropriate actions on the data and the targets\n",
        "    # the format of data should be in [0, 1]\n",
        "    image = self.data[index]\n",
        "    input = torch.unsqueeze(image,0) #add channel dim at position 0\n",
        "\n",
        "    target = self.target_vectors[index]\n",
        "    return input, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvmK-kmdm0L6"
      },
      "source": [
        "### Test 2: Data Sets\n",
        "\n",
        "\n",
        "Instantiate the training dataset.\n",
        "Implement a data loader for the training dataset with a batch size of 64.\n",
        "Assure that all inputs are of the desired type and shape.\n",
        "Assert that the target values are in the correct format, and the sum of the target values per sample is one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgFrIjoom0L6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99d90ca-b3e9-4d52-df1b-dcdeefa1c430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/DataSet/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4588202.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/DataSet/raw/train-images-idx3-ubyte.gz to ./data/DataSet/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/DataSet/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 133983.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/DataSet/raw/train-labels-idx1-ubyte.gz to ./data/DataSet/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/DataSet/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1281273.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/DataSet/raw/t10k-images-idx3-ubyte.gz to ./data/DataSet/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/DataSet/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6055476.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/DataSet/raw/t10k-labels-idx1-ubyte.gz to ./data/DataSet/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# instantiate the training dataset\n",
        "\n",
        "train_set = DataSet(purpose=\"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_set, 64, shuffle=True)\n",
        "\n",
        "# assert that we have not filtered out all samples\n",
        "assert len(train_set)!= 60000 and len(train_set)== 48133\n",
        "\n",
        "# check the batch and assert valid data and sizes\n",
        "for x,t in train_loader:\n",
        "  assert len(x) <= 64\n",
        "  assert len(t) == len(x)\n",
        "  assert torch.all(torch.sum(t, axis = 1) == 1)\n",
        "  assert x.shape == torch.Size([x.shape[0], 1, 28, 28])\n",
        "  assert x.dtype == torch.float32\n",
        "  assert torch.max(x) <= 1\n",
        "  assert torch.min(x) >= 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-7VFbW1IfVl"
      },
      "source": [
        "### Task 4: Data Loader\n",
        "\n",
        "Call the dataset class sperately with batch size of $B=256$, and instantiate data loaders for the three datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svg_iiEyIfms"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 256\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# instantiate training dataset and data loader\n",
        "train_set = DataSet(\"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# instantiate validation set and data loader\n",
        "validation_set = DataSet(\"validation\")\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size)\n",
        "\n",
        "# instantiate test set and according data loader\n",
        "test_set = DataSet(\"test\")\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W67k2w9Mm0L7"
      },
      "source": [
        "### Task 5: Utility Function\n",
        "\n",
        "Implement a function that splits a batch of samples into known and unknown parts. For the known parts, also provide the target vectors.\n",
        "How can we know which of the data samples are known samples, and which are unknown?\n",
        "\n",
        "This function needs to return three elements:\n",
        "* First, the samples from the batch that belong to known classes.\n",
        "* Second, the target vectors that belong to the known classes.\n",
        "* Finally, the samples from the batch that belong to unknown classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "PSDfc-2Tm0L7"
      },
      "outputs": [],
      "source": [
        "def split_known_unknown(batch, targets):\n",
        "    known_samples = []\n",
        "    known_targets = []\n",
        "    unknown_samples = []\n",
        "\n",
        "    for sample, target in zip(batch, targets):\n",
        "        are_all_equal = torch.all(torch.eq(target, 1/4))\n",
        "        if  are_all_equal:\n",
        "          unknown_samples.append(sample)\n",
        "        else:\n",
        "          known_samples.append(sample)\n",
        "          known_targets.append(target)\n",
        "\n",
        "\n",
        "\n",
        "    known_samples = torch.stack(known_samples)\n",
        "    known_targets = torch.stack(known_targets)\n",
        "    unknown_samples = torch.stack(unknown_samples)\n",
        "\n",
        "    return known_samples, known_targets, unknown_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVq2US6am0L8"
      },
      "source": [
        "## Loss Function and Confidence\n",
        "\n",
        "We write our own PyTorch implementation of our loss function.\n",
        "Particularly, we implement a manual way to define the derivative of our loss function via `torch.autograd.Function`, which allows us to define the forward and backward pass on our own.\n",
        "For this purpose, we need to implement two `static` functions in our loss.\n",
        "The function `forward(ctx, logits, targets)` is required to compute the loss value and allows us to store some variables in the context of the backward pass.\n",
        "The `backward(ctx, result)` provides us with the result of the forward function (the loss value) as well as the context with our stored variables.\n",
        "Here, we need to compute the derivative of the loss with respect to both of the inputs to the forward function (which might look a bit confusing), i.e.,$\\frac{\\partial \\mathbf{J}^{CCE}}{\\partial \\mathbf{Z}}$ and $\\frac{\\partial \\mathbf{J}^{CCE}}{\\partial \\mathbf{T}}$.\n",
        "Since the latter is not required, we can also simply return `None` for the second derivative.\n",
        "\n",
        "<font color=#FF000>Hint: if you think the implementation of loss function is too hard, you can also cross-entropy as your loss function (**since PyTorch version 1.11**).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oV8dE2u5B-i"
      },
      "source": [
        "### Task 6: Loss Function Implementation\n",
        "\n",
        "Implement a `torch.autograd.Function` class for the adapted SoftMax function according to the equations provided in the lecture.\n",
        "You might want to compute the log of the network output $\\ln y_o$ from the logits $z_o$ via `torch.nn.functional.log_softmax`.\n",
        "Store all the data required for the backward pass in the context during `forward`, and extract these from the context during `backward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czmU2Aqqm0L8"
      },
      "outputs": [],
      "source": [
        "class AdaptedSoftMax(torch.autograd.Function):\n",
        "\n",
        "  # implement the forward propagation\n",
        "  @staticmethod\n",
        "  def forward(ctx, logits, targets):\n",
        "     log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n",
        "     ctx.save_for_backward(log_probs, targets)\n",
        "     loss = -(log_probs * targets).sum(dim=1).mean()\n",
        "     return loss\n",
        "\n",
        "  # implement Jacobian\n",
        "  @staticmethod\n",
        "  def backward(ctx, result):\n",
        "    log_probs, targets = ctx.saved_tensors\n",
        "    y = torch.exp(log_probs)\n",
        "    dJ_dz = y - targets\n",
        "    return dJ_dz, None\n",
        "\n",
        "# DO NOT REMOVE!\n",
        "# here we set the adapted softmax function to be used later\n",
        "adapted_softmax = AdaptedSoftMax.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eopp5YC6m0L9"
      },
      "source": [
        "### Task 6a: Alternative Loss Function\n",
        "\n",
        "In case the loss function is too difficult to implement, you can also choose to rely on PyTorch's automatic gradient computation and simply define your loss function without the backward pass.\n",
        "\n",
        "In this case, we only need to define the forward pass. A simple function `adapted_softmax(logits, targets)` is sufficient.\n",
        "\n",
        "You can implement any variant of the categorical cross-entropy loss function on top of SoftMax activations as defined in the lecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "fFhw66Pxm0L9"
      },
      "outputs": [],
      "source": [
        "def adapted_softmax_alt(logits, targets):\n",
        "    # compute cross-entropy loss on top of softmax values of the logits\n",
        "    softmax_logits = torch.nn.functional.softmax(logits, dim=1)\n",
        "    loss = torch.mean(-torch.sum(targets * torch.log(softmax_logits), dim=1))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDwhq72Vm0L9"
      },
      "source": [
        "### Task 7: Confidence Evaluation\n",
        "\n",
        "Implement a function to compute the confidence value for a given batch of samples.\n",
        "Compute Softmax confidence and split these confidences between known and unknown classes.\n",
        "For samples from known classes, sum up the SoftMax confidences of the correct class.\n",
        "For unknown samples, sum 1 minus the maximum confidence for any of the known classes; also apply the $\\frac1O$ correction for the minimum possible SoftMax confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-ooydMNsm0L9"
      },
      "outputs": [],
      "source": [
        "def confidence(logits, targets):\n",
        "    o = targets.shape[1]\n",
        "    # compute softmax confidences\n",
        "    softmax_logits = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    # split between known and unknown\n",
        "    unknown_mask = torch.all(targets == 1/4, dim=1)\n",
        "    known_mask = ~unknown_mask\n",
        "    # compute confidence score for known targets\n",
        "    conf_known = torch.sum(softmax_logits[known_mask, torch.argmax(targets[known_mask], dim=1)])\n",
        "    # compute confidence score for unknown targets\n",
        "    #conf_unknown = torch.sum(1 - torch.max(softmax_logits[unknown_mask], dim=1)[0])\n",
        "    conf_unknown = torch.sum(1 - torch.max(softmax_logits[unknown_mask], dim=1)[0] + 1/o)\n",
        "\n",
        "    return conf_known + conf_unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e543YdEvm0L-"
      },
      "source": [
        "### Test 3: Check Confidence Implementation\n",
        "\n",
        "Test that your confidence implementation does what it is supposed to do.\n",
        "\n",
        "Note that confidence values should always be between 0 and 1, other values indicate an issue in the implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select good logit vectors for known and unknown classes\n",
        "logits = torch.tensor([[10., 0., 0., 0.], [-10., 0, -10., -10.], [0.,0.,0.,0.]])\n",
        "# select the according target vectors for these classes\n",
        "targets = torch.stack([target_vector(known_classes[0]), target_vector(known_classes[1]), target_vector(negative_classes[0])])\n",
        "# the confidence should be close to 1 for all cases\n",
        "assert 3 - confidence(logits, targets) < 1e-3"
      ],
      "metadata": {
        "id": "n4yZVEbJq66x"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWFgh2Nwm0L-"
      },
      "source": [
        "## Network and Training\n",
        "\n",
        "We make use of the same convolutional network as utilized in Assignment 6, to which we append a final fully-connected layer with $K$ inputs and $O$ outputs.\n",
        "Additionally, we replace the $\\sigma$ activation function with the better-performing **PReLU** function.\n",
        "\n",
        "The topology can be found in the following:\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0\n",
        "2. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
        "3. activation function **PReLU**\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
        "5. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
        "6. activation function **PReLU**\n",
        "7. flatten layer to convert the convolution output into a vector\n",
        "8. fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "9. fully-connected layer with $K$ inputs and $O$ outputs\n",
        "\n",
        "\n",
        "\n",
        "However, instead of relying on the `torch.nn.Sequential` class, we need to define our own network class, which we need to derive from `torch.nn.Module` -- since our network has two outputs.\n",
        "We basically need to implement two methods in our network.\n",
        "The constructor `__init__(self, Q1, Q2, K)` needs to call the base class constructor and initialize all required layers of our network.\n",
        "The `forward(self, x)` function then passes the input through all of our layers and returns both the deep features (extracted at the first fully-connected layer) and the logits (extracted from the second fully-connected layer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAo2Qx9q5bk8"
      },
      "source": [
        "\n",
        "### Task 8: Network Definition\n",
        "\n",
        "We define our own small-scale network to classify known and unknown samples for MNIST.\n",
        "\n",
        "However, this time we need to implement our own network model since we need to modify our network output.\n",
        "\n",
        "Implement a network class, including the layers as provided above.\n",
        "Implement both the constructor and the forward function.\n",
        "Instantiate the network with $Q_1=32, Q_2=32, K=20, O=4$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "LYyPEmsWm0L-"
      },
      "outputs": [],
      "source": [
        "class Network (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K, O):\n",
        "    # call base class constrcutor\n",
        "    super(Network,self).__init__()\n",
        "    # define convolutional layers\n",
        "    self.conv1 = torch.nn.Conv2d(1, Q1, kernel_size=7, stride=1, padding=0)\n",
        "    self.conv2 = torch.nn.Conv2d(Q1, Q2, kernel_size=5, stride=1, padding=2)\n",
        "    # pooling and activation functions will be re-used for the different stages\n",
        "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.act = torch.nn.PReLU()\n",
        "    # define fully-connected layers\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    self.fc1 = torch.nn.Linear(Q2 * 5 * 5, K)\n",
        "    self.fc2 = torch.nn.Linear(K, O)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # compute first layer of convolution, pooling and activation\n",
        "    a = self.conv1(x)\n",
        "    a = self.pool(a)\n",
        "    a = self.act(a)\n",
        "    # compute second layer of convolution, pooling and activation\n",
        "    a = self.conv2(a)\n",
        "    a = self.pool(a)\n",
        "    a = self.act(a)\n",
        "\n",
        "    # get the deep features as the output of the first fully-connected layer\n",
        "    deep_features = self.fc1(self.flatten(a))\n",
        "    # get the logits as the output of the second fully-connected layer\n",
        "    logits = self.fc2(deep_features)\n",
        "    # return both the logits and the deep features\n",
        "    return logits, deep_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rxnrxtRm0L-"
      },
      "source": [
        "### Task 9: Training Loop\n",
        "\n",
        "Implement a function for training network, which contains the training and validation loop.\n",
        "Compute the training set confidence during the epoch.\n",
        "At the end of each epoch, also compute the validation set confidence measure.\n",
        "Print both the training set and validation set confidence scores to the console.\n",
        "Finally, return the trained network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "wnLIrcTzm0L_"
      },
      "outputs": [],
      "source": [
        "def train(network,epochs, eta, momentum, loss_function):\n",
        "\n",
        "  # Set GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  network.to(device)\n",
        "\n",
        "  # SGD optimizer with appropriate learning rate\n",
        "  optimizer = torch.optim.SGD(network.parameters(), lr=eta, momentum=momentum)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # evaluate average confidence for training and validation set\n",
        "    train_conf = validation_conf = 0.\n",
        "\n",
        "    for x,t in train_loader:\n",
        "      x, t = x.to(device), t.to(device)\n",
        "\n",
        "      # extract logits (and deep features) from network\n",
        "      x, t = x.to(device), t.to(device)\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      logits, _ = network(x)\n",
        "      # compute loss\n",
        "      loss = loss_function(logits, t)\n",
        "      # perform weight update\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # compute training confidence\n",
        "      train_conf += confidence(logits, t)\n",
        "\n",
        "    # compute validation comfidence\n",
        "    with torch.no_grad():\n",
        "      for x_val,t_val in validation_loader:\n",
        "        x_val, t_val = x_val.to(device), t_val.to(device)\n",
        "        # extract logits (and deep features)\n",
        "        x_val, t_val = x_val.to(device), t_val.to(device)\n",
        "        # compute validation confidence\n",
        "        logits_val, _ = network(x_val)\n",
        "        validation_conf += confidence(logits_val, t_val)\n",
        "    # print average confidence for training and validation\n",
        "    print(f\"\\rEpoch {epoch}; train: {train_conf/len(train_set):1.5f}, val: {validation_conf/len(validation_set):1.5f}\")\n",
        "\n",
        "  return network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MVLbae8RErZ"
      },
      "source": [
        "### Task 10: Network training\n",
        "\n",
        "Instantiate network with $K1=K2=32$, $K=20$ and $O = 4$. Train the network for 30 epochs with an appropriate learning rate (the optimal learning rate might depend on your loss function implementation and can vary between 0.1 and 0.00001), momentum=$0.9$, and call the function you defined in Task 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "JiZKtxFdHaJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d977205a-8b63-4e56-c3ba-a28c9291c8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0; train: 0.60027, val: 0.60057\n",
            "Epoch 1; train: 0.60084, val: 0.60115\n",
            "Epoch 2; train: 0.60146, val: 0.60175\n",
            "Epoch 3; train: 0.60202, val: 0.60230\n",
            "Epoch 4; train: 0.60258, val: 0.60285\n",
            "Epoch 5; train: 0.60313, val: 0.60341\n",
            "Epoch 6; train: 0.60368, val: 0.60395\n",
            "Epoch 7; train: 0.60421, val: 0.60450\n",
            "Epoch 8; train: 0.60478, val: 0.60506\n",
            "Epoch 9; train: 0.60531, val: 0.60557\n",
            "Epoch 10; train: 0.60583, val: 0.60609\n",
            "Epoch 11; train: 0.60637, val: 0.60663\n",
            "Epoch 12; train: 0.60687, val: 0.60712\n",
            "Epoch 13; train: 0.60738, val: 0.60763\n",
            "Epoch 14; train: 0.60788, val: 0.60813\n",
            "Epoch 15; train: 0.60837, val: 0.60862\n",
            "Epoch 16; train: 0.60885, val: 0.60910\n",
            "Epoch 17; train: 0.60933, val: 0.60958\n",
            "Epoch 18; train: 0.60982, val: 0.61006\n",
            "Epoch 19; train: 0.61030, val: 0.61053\n",
            "Epoch 20; train: 0.61075, val: 0.61099\n",
            "Epoch 21; train: 0.61121, val: 0.61145\n",
            "Epoch 22; train: 0.61168, val: 0.61191\n",
            "Epoch 23; train: 0.61214, val: 0.61238\n",
            "Epoch 24; train: 0.61261, val: 0.61283\n",
            "Epoch 25; train: 0.61304, val: 0.61327\n",
            "Epoch 26; train: 0.61350, val: 0.61373\n",
            "Epoch 27; train: 0.61397, val: 0.61418\n",
            "Epoch 28; train: 0.61439, val: 0.61460\n",
            "Epoch 29; train: 0.61482, val: 0.61503\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "K1 = K2 = 32\n",
        "K = 20\n",
        "O = 4\n",
        "epochs = 30\n",
        "eta = 0.00001  # Adjust the learning rate based on your loss function\n",
        "momentum = 0.9\n",
        "\n",
        "# Instantiate the network\n",
        "network_adapted = Network(K1, K2, K, O)\n",
        "\n",
        "# Train the network\n",
        "network_adapted = train(network_adapted, epochs, eta, momentum, adapted_softmax_alt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDqRH6Bsm0L_"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "For evaluation, we test two different things.\n",
        "First, we check whether our intuition is correct, and the training helps reduce the deep feature magnitudes of unknown samples while maintaining magnitudes for known samples.\n",
        "It is also interesting to see whether there is a difference between samples of the negative classes that were seen during training, and unknown classes that were not.\n",
        "For this purpose, we extract the deep features for the validation and test sets, compute their magnitudes, and plot them in a histogram.\n",
        "\n",
        "The second evaluation computes Correct Classification Rates (CCR) and False Positive Rates (FPR) for a given confidence threshold $\\zeta=0.98$ (based on your training results, you might want to vary this threshold).\n",
        "For the known samples, we compute how often the correct class was classified with a confidence over threshold.\n",
        "For unknown samples, we assess how often one of the known classes was predicted with a confidence larger than $\\zeta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnwwGYj17g2J"
      },
      "source": [
        "### Task 11: Feature Magnitude Plot\n",
        "\n",
        "Define a function, which can extract deep features for validation and test set samples and compute their magnitudes. Split them into known, negative (validation set), and unknown (test set). Plot a histogram for each of the three types of samples.\n",
        "Note that the minimum magnitude is 0, and the maximum magnitude can depend on your network training success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iM0SayyKm0L_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "9c340dc0-dd22-472f-c209-23f35cbde60f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'network_adapted' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1e7f92f5d7ed>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mplot_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_adapted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'network_adapted' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTSMfeJ_m0L_"
      },
      "source": [
        "### Task 12: Classification Evaluation\n",
        "\n",
        "For a fixed threshold of $\\zeta=0.98$, compute CCR and FPR for the test set.\n",
        "A well-trained network can achieve a CCR of > 90% for an FPR < 10%.\n",
        "You might need to vary the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlrBZlHxm0L_"
      },
      "outputs": [],
      "source": [
        "def evaluation(network):\n",
        "  zeta = 0.98\n",
        "\n",
        "  # count the correctly classified and the total number of known samples\n",
        "  correct = known = 0\n",
        "  # count the incorrectly classified and the total number of unknown samples\n",
        "  false = unknown = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,t in test_loader:\n",
        "      # extract logits (and deep features)\n",
        "      ...\n",
        "      # compute softmax confidences\n",
        "      ...\n",
        "      # split between known and unknown\n",
        "      ...\n",
        "\n",
        "      # compute number of correctly classified knowns above threshold\n",
        "      correct += ...\n",
        "      known += ...\n",
        "\n",
        "      # compute number of incorrectly accepted known samples\n",
        "      false += ...\n",
        "      unknown += ...\n",
        "\n",
        "  # print both rates\n",
        "  print (f\"CCR: {correct} of {known} = {correct/known*100:2.2f}%\")\n",
        "  print (f\"FPR: {false} of {unknown} = {false/unknown*100:2.2f}%\")\n",
        "\n",
        "\n",
        "evaluation(...)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}