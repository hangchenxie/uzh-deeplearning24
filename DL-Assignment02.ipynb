{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Hangchen Xie, 22-736-789\n",
    "- Yanyang Gong, 23-744-063\n",
    "- Yaojie Wang, 23-741-283"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Gradient Descent\n",
    "\n",
    "The goal of this exercise is to gain experience with a basic technique of Deep Learning, i.e., gradient descent.\n",
    "A two-dimensional loss surface is created manually and gradient descent is implemented.\n",
    "Several runs of gradient descent from different starting locations will be performed.\n",
    "The loss surface and the detected minima are plotted together in one 3D plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aKzqZe_UgdKf"
   },
   "source": [
    "## Compute the Gradient\n",
    "The loss function is manually defined as $$\\mathcal J_{\\vec w}=40 \\sin^3\\left(\\frac{1}{2}w_1+ w_2\\right) + w_1^2 + w_2^2$$\n",
    "The weights $\\vec w = (w_1, w_2)^T$ shall be optimized such that the loss function has a minimum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NX0qB7Emgf2b"
   },
   "source": [
    "### Task 1: Compute the Gradient\n",
    "\n",
    "The gradient $\\nabla \\mathcal J_{\\vec w}$ is defined as the partial derivatives of the loss function with respect to the two variables $w_1$ and $w_2$.\n",
    "We need to calculate it:\n",
    "\n",
    "* $\\frac{\\partial \\mathcal J}{\\partial w_1} = 60 \\sin^2\\left(\\frac{1}{2}w_1+ w_2\\right)\\cos\\left(\\frac{1}{2}w_1+ w_2\\right)  + 2w_1$\n",
    "* $\\frac{\\partial \\mathcal J}{\\partial w_2} = 120 \\sin^2\\left(\\frac{1}{2}w_1+ w_2\\right)\\cos\\left(\\frac{1}{2}w_1+ w_2\\right)   + 2w_2$"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed gradient w.r.t w1: 2*w1 + 60*sin(w1/2 + w2)**2*cos(w1/2 + w2)\n",
      "Computed gradient w.r.t w2: 2*w2 + 120*sin(w1/2 + w2)**2*cos(w1/2 + w2)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# define the symbols\n",
    "w1, w2 = sp.symbols('w1 w2')\n",
    "\n",
    "# define the loss function\n",
    "J = 40 * sp.sin((w1/2) + w2)**3 + w1**2 + w2**2\n",
    "\n",
    "# compute the gradients\n",
    "grad_w1 = sp.diff(J, w1)\n",
    "grad_w2 = sp.diff(J, w2)\n",
    "\n",
    "print(\"Computed gradient w.r.t w1:\", grad_w1)\n",
    "print(\"Computed gradient w.r.t w2:\", grad_w2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:51:17.674133Z",
     "start_time": "2024-03-02T14:51:15.563832Z"
    }
   },
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sGZi-Gs5ghic"
   },
   "source": [
    "### Task 2: Implement the Loss Function\n",
    "\n",
    "Implement the loss function in Python, which takes a given $\\vec w$ and returns $\\mathcal J_{\\vec w}$ according to the given loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XqRlmAfxhMCP",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:51:18.049338Z",
     "start_time": "2024-03-02T14:51:17.676110Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "def loss(w):\n",
    "    return 40 * numpy.sin((w[0]/2) + w[1])**3 + w[0]**2 + w[1]**2\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_d2XYSHMhMCQ"
   },
   "source": [
    "### Task 3: Implement the Gradient\n",
    "\n",
    "Implement the gradient as a function in Python, which takes a given $\\vec w$ and returns $\\nabla\\mathcal J_{\\vec w}$ according to the analytical result in Task 1.\n",
    "Remember that the gradient needs to be computed and returned for both $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MYvhZJnDhMCQ",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:51:18.064862Z",
     "start_time": "2024-03-02T14:51:18.051334Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(w):\n",
    "    grad_w1 = 60 * numpy.sin((w[0]/2) + w[1])**2 * numpy.cos((w[0]/2) + w[1]) + 2*w[0]\n",
    "    grad_w2 = 120 * numpy.sin((w[0]/2) + w[1])**2 * numpy.cos((w[0]/2) + w[1]) + 2*w[1]\n",
    "    return numpy.array([grad_w1, grad_w2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LcMfR2kX69Xe"
   },
   "source": [
    "### Test 1: Test Functions\n",
    "The codes below call the loss function from Task 2 and the gradient function from Task 3 with $\\vec w=(0,0)^T$ and then compare the return values with the given analytically computed values.\n",
    "Please check your implementation if the tests cannot be passed.\n",
    "\n",
    "Make sure your code can pass the test before moving to the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ueZTSSTIhMCR",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:51:18.079863Z",
     "start_time": "2024-03-02T14:51:18.067861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed\n"
     ]
    }
   ],
   "source": [
    "w = numpy.zeros(2)\n",
    "\n",
    "# analytically compute the expected values\n",
    "expected_loss = 0.\n",
    "expected_gradient = numpy.array((0.,0.))\n",
    "\n",
    "# test loss function\n",
    "assert abs(loss(w) - expected_loss) < 1e-8\n",
    "assert numpy.all(numpy.abs(gradient(w) - expected_gradient) < 1e-8)\n",
    "print(\"Tests passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Gxe3nIhnhMCR"
   },
   "source": [
    "## Implement Gradient Descent\n",
    "The procedure of gradient decent is the repeated application of two steps:\n",
    "\n",
    "* First, the gradient of the loss $\\nabla\\mathcal J_{\\vec w}$ is computed based on the current value of the parameters $\\vec w$.\n",
    "\n",
    "* Second, the weights are updated by moving a small step in the direction of the negative gradient: $\\vec w = \\vec w - \\eta\\nabla\\mathcal J_{\\vec w}$\n",
    "\n",
    "Optionally, the loss $\\mathcal J_{\\vec w}$ is computed to record the progress of the gradient descent.\n",
    "Finally, one or more appropriate criteria need to be defined to decide when to stop the procedure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-NjDp5ldgxmZ"
   },
   "source": [
    "### Task 4: Termination Criterion\n",
    "\n",
    "(theoretical question) Define a proper termination criterion. Which error cases might occur and need to be considered?\n",
    "\n",
    "Termination criterion: The termination criterion should be a threshold for the norm of the gradient. If the norm of the gradient is smaller than the threshold and little change in the loss after fixed number of iterations, the algorithm should stop.\n",
    "\n",
    "Error Cases:\n",
    "1. Reaches local minimum: The algorithm may stop at a local minimum instead of the global minimum.\n",
    "2. Gets suck in plateau: The algorithm may stop at a plateau where the gradient is very small.\n",
    "3. Oscillates / diverges: The algorithm may oscillate around the minimum or diverge if the learning rate is too large.\n",
    "4. Jumps over the minimum: The algorithm may jump over the minimum if the learning rate is too large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew3iZPupgymq"
   },
   "source": [
    "### Task 5: Implement Gradient Descent\n",
    "\n",
    "Implement a function that performs the gradient descent. This function should take as parameters an initial weight vector $\\vec w$ and a learning rate $\\eta$, and make use of the gradient function implemented in Task 3 and, possibly, the loss function from Task 2.\n",
    "It should return the optimized weight vector $\\vec w^*$. Incorporate the termination criterion designed in Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XBGsNgPYhMCS",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:51:18.095862Z",
     "start_time": "2024-03-02T14:51:18.083862Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(w, eta=0.01):\n",
    "  # copy the weights to not modify the original values\n",
    "  w_star = w.copy()\n",
    "\n",
    "  # perform iterative gradient descent\n",
    "  while True:\n",
    "    # compute the gradient\n",
    "    grad = gradient(w_star)\n",
    "\n",
    "    # update the weights\n",
    "    w_star -=  eta * grad\n",
    "\n",
    "    # include additional termination criteria?\n",
    "    # if loss(w_star) < 0:\n",
    "    #    w_star  -= 1.1 * eta * grad\n",
    "    # else:\n",
    "    #    w_star  -= 0.5 * eta * grad\n",
    "    if loss(w_star) < 1e-8:\n",
    "      break\n",
    "      \n",
    "\n",
    "  return w_star"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e83uiKEGhGi7"
   },
   "source": [
    "## Evaluate Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ8BZ3B8hMCS"
   },
   "source": [
    "### Task 6: Evaluate Gradient Descent\n",
    "Call the gradient descent function from Task 5 1000 times with different random initialized weights $\\vec w\\in[-10,10]^2$ and a learning rate of $\\eta=0.01$. Store the resulting optimized weight vectors in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MLCz_rizhMCT",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:53:48.925660Z",
     "start_time": "2024-03-02T14:51:18.097862Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m w \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# call gradient descent\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m w_star \u001B[38;5;241m=\u001B[39m \u001B[43mgradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# store it in the list\u001B[39;00m\n\u001B[0;32m      9\u001B[0m stored_weights\u001B[38;5;241m.\u001B[39mappend(w_star)\n",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m, in \u001B[0;36mgradient_descent\u001B[1;34m(w, eta)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# perform iterative gradient descent\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m      7\u001B[0m   \u001B[38;5;66;03m# compute the gradient\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m   grad \u001B[38;5;241m=\u001B[39m \u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw_star\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m   \u001B[38;5;66;03m# update the weights\u001B[39;00m\n\u001B[0;32m     11\u001B[0m   w_star \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m  eta \u001B[38;5;241m*\u001B[39m grad\n",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m, in \u001B[0;36mgradient\u001B[1;34m(w)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgradient\u001B[39m(w):\n\u001B[0;32m      2\u001B[0m     grad_w1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m60\u001B[39m \u001B[38;5;241m*\u001B[39m numpy\u001B[38;5;241m.\u001B[39msin((w[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m+\u001B[39m w[\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m numpy\u001B[38;5;241m.\u001B[39mcos((w[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m+\u001B[39m w[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39mw[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m     grad_w2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m120\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m numpy\u001B[38;5;241m.\u001B[39mcos((w[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m+\u001B[39m w[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39mw[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39marray([grad_w1, grad_w2])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stored_weights = []\n",
    "\n",
    "for i in range(1000):\n",
    "  # create random weight vector\n",
    "  w = numpy.random.uniform(-10, 10, 2)\n",
    "  # call gradient descent\n",
    "  w_star = gradient_descent(w, 0.01)\n",
    "  # store it in the list\n",
    "  stored_weights.append(w_star)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JOVpknxZ5cU_"
   },
   "source": [
    "### Test 2: Check Minima\n",
    "\n",
    "Counting the number of local minima in our loss function, we reach a total of 11. Please use this function to verify that your implementation could reach this number at maximum.\n",
    "\n",
    "Again, make sure you pass the test before moving to the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTth8eW3hMCT",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:53:48.927660Z",
     "start_time": "2024-03-02T14:53:48.927660Z"
    }
   },
   "outputs": [],
   "source": [
    "maximum_number_of_minima = 11\n",
    "\n",
    "# compute the number of reached minima\n",
    "minima = []\n",
    "for w_star in stored_weights:\n",
    "  # check if this weight vector is far enough\n",
    "  # from all previously stored vectors\n",
    "  if all(numpy.linalg.norm(w_star-w) > 1e-3 for w in minima):\n",
    "    minima.append(w_star)\n",
    "number_of_minima = len(minima)\n",
    "\n",
    "assert number_of_minima <= maximum_number_of_minima\n",
    "\n",
    "print(\"Check passed. The number of minima\", number_of_minima, \"is lower than or equal to the maximum\", maximum_number_of_minima)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "q8SUKqAghMCT"
   },
   "source": [
    "### Task 7: Find the Global Minimum\n",
    "\n",
    "Find the global minimum of our error function by evaluating the obtained optimized weight vectors from Task 6.\n",
    "Print the minimum and its loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF9ExVgwhMCU",
    "ExecuteTime": {
     "end_time": "2024-03-02T14:53:48.929660Z",
     "start_time": "2024-03-02T14:53:48.928661Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the lowest loss\n",
    "minimum_loss = numpy.inf\n",
    "minimum_weights = None\n",
    "\n",
    "for w_star in stored_weights:\n",
    "  J = loss(w_star)\n",
    "  if J < minimum_loss:\n",
    "    minimum_loss = J\n",
    "    minimum_weights = w_star\n",
    "\n",
    "print(\"The minimum loss value of:\", minimum_loss, \"was found for minimum\", minimum_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nxuktyA-hOCo"
   },
   "source": [
    "## Plot Error Surface and Points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bj6cRLiRhMCU"
   },
   "source": [
    "### Task 8: Loss Surface Plot\n",
    "\n",
    "Plot the error surface of the given loss function. Limit range $\\vec w\\in[-20,20]^2$. For each of the optimized weights from Task 6, plot a marker into the 3D plot. An example can be found in the slides.\n",
    "\n",
    "When plotting the resulting optimized weights $\\vec w=(w_1, w_2)^T$, we need to define the third coordinate. What should this coordinate be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmR3sdirhMCU",
    "ExecuteTime": {
     "start_time": "2024-03-02T14:53:48.930661Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "# create 3D axis\n",
    "figure = pyplot.figure()\n",
    "axis = figure.add_subplot(111, projection='3d', azim = -40, elev=50)\n",
    "\n",
    "# define range to plot\n",
    "w_range = numpy.linspace(-20, 20, 100)\n",
    "w1, w2 = numpy.meshgrid(w_range, w_range)\n",
    "\n",
    "# compute loss for w1 and w2\n",
    "J = loss([w1, w2])\n",
    "\n",
    "# plot surface with jet colormap\n",
    "axis.plot_surface(w1, w2, J, cmap=\"jet\", alpha=0.7)\n",
    "\n",
    "# plot resulting points in 3D\n",
    "for w_star in stored_weights:\n",
    "  # compute the z-position\n",
    "  z = loss(w_star)\n",
    "  # plot as 3D point\n",
    "  axis.plot([w_star[0]], [w_star[1]], [z], \"kx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
